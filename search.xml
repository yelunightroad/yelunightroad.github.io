<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TensorFlow中tfRecord的使用以及测试]]></title>
    <url>%2F2018%2F04%2F09%2Ftensorflow%E4%B8%ADtfRecord%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BB%A5%E5%8F%8A%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[tfRecord是TensorFlow官方推荐的标准数据格式，对于存储字典类型的数据比较友好，所以撰写此文记录一下使用过程中的一点经验。 tfRecord结构tfRecord中存储的记录由tf.train.Example定义，其实就是一个protocol buffer文件，定义如下： 123456789101112131415message Example &#123; Features features = 1;&#125;;message Features&#123; map&lt;string,Feature&gt; featrue = 1;&#125;;message Feature&#123; oneof kind&#123; BytesList bytes_list = 1; FloatList float_list = 2; Int64List int64_list = 3; &#125;&#125;; 可以看到其主要包含了一个字符串格式的key，以及字符串、浮点数或者整数类型的列表构成的value 代码示例12345678import tensorflow as tfimport oskeys=[[1.0,2.0],[2.0,3.0],[1,2]]keys2=[[1.0,2.0],[2.0,3.0],[3.0]]strtest='aaa'sess=tf.InteractiveSession()sess.run(tf.global_variables_initializer()) 基础变量，没什么可解释的 1234567891011121314151617181920212223def make_example(key): example = tf.train.Example(features=tf.train.Features( feature=&#123; 'ft':tf.train.Feature(float_list=tf.train.FloatList(value=key)), 'st':tf.train.Feature(bytes_list=tf.train.BytesList(value=[strtest])) &#125; )) return examplefilename="tmp.tfrecords"if os.path.exists(filename): os.remove(filename)writer = tf.python_io.TFRecordWriter(filename)for key in keys: ex = make_example(key) writer.write(ex.SerializeToString())writer.close()reader = tf.TFRecordReader()filename_queue = tf.train.string_input_producer(["tmp.tfrecords"],num_epochs=2)_,serialized_example =reader.read(filename_queue)batch = tf.train.batch(tensors=[serialized_example],batch_size=3) 在以上代码中我们展示了如何定义一个example并将其写入文件，同事19行开始我们也对如何读取文件做了简单示例，现在读取部分也可以使用dataset，更加符合官方潮流，例如用一下代码代替19-23行 12dataset = tf.data.TFRecordDataset(filenames=filenames, buffer_size=200).batch(3)record_iter = dataset.make_one_shot_iterator() 下面进入重头戏，如何解析tfRecord的内容 12345678910features=&#123; "ft":tf.VarLenFeature(tf.float32), "st":tf.VarLenFeature(tf.string)&#125;key_parsed = tf.parse_example(batch,features)sess.run(tf.initialize_local_variables())print tf.contrib.learn.run_n(key_parsed) #tf.train.get_or_create_global_step()#print tf.contrib.training.train(key_parsed,'../') 输出如下 12345678[&#123;&apos;ft&apos;: SparseTensorValue(indices=array([[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]]), values=array([1., 2., 2., 3., 1., 2.], dtype=float32), dense_shape=array([3, 2])), &apos;st&apos;: SparseTensorValue(indices=array([[0, 0], [1, 0], [2, 0]]), values=array([&apos;aaa&apos;, &apos;aaa&apos;, &apos;aaa&apos;], dtype=object), dense_shape=array([3, 1]))&#125;] 其实非常简单，定义一个同样的feature，然后用相应的方法获取就可以了，注意代码的第8行的方法已经废弃了，TensorFlow相关人员表示没有开发其替代函数的打算，想实现相应的效果可以使用第9、10行的代码，基本可以替代，但是做实验的话由于train方法其实是在训练，还是8行的函数简单些，在删除前我们暂且继续使用。 在以上代码中我们使用的是VarlenFeature，它更通用一些，另外比较常见的还有FixedLenFeature，使用代码如下 1234567features=&#123; &quot;ft&quot;:tf.FixedLenFeature(shape=[2],dtype=tf.float32,default_value=[2.0,3.0])&#125;key_parsed = tf.parse_example(batch,features)print tf.contrib.learn.run_n(key_parsed) 输出结果为 123[&#123;&apos;ft&apos;: array([[1., 2.], [2., 3.], [1., 2.]], dtype=float32)&#125;] 这里需要注意的是它解析的数据必须是定长的，且数据格式与shape参数必须一致，否则就会报错。另外它返回的是一个tensor不同于VarLenFeature返回的是sparseTensor。这里default_value的意义是你查询的字段不存在时赋予默认值，比如你现在查询的是ft字段，如果存储时只存储了st那么就会使用默认值赋值，所以default_value，的shape必须与shape参数一致。 有了以上两个参数基本可以解决我们的数据读写问题，如果还不够，可以了解一下SparseFeature，它可以以一种类似SparseTensor的方式帮你定义数据 1`serialized`: [ features { feature { key: “val” value { float_list { value: [ 0.5, -1.0 ] } } } feature { key: “ix” value { int64_list { value: [ 3, 20 ] } } } }, features { feature { key: “val” value { float_list { value: [ 0.0 ] } } } feature { key: “ix” value { int64_list { value: [ 42 ] } } } } ] 1And arguments example_names: [“input0”, “input1”], features: { “sparse”: SparseFeature( index_key=”ix”, value_key=”val”, dtype=tf.float32, size=100), } 12345678Then the output is a dictionary:```python&#123; &quot;sparse&quot;: SparseTensor( indices=[[0, 3], [0, 20], [1, 42]], values=[0.5, -1.0, 0.0] dense_shape=[2, 100]),&#125; 123456789101112## 其它相关函数除了以上用到的parse_example方法，还有一些其它方法我们可以简单了解。### tf.parse_single_example该方法与tf.parse_example方法基本相同，只不过少了batch函数，单独解析### tf.parse_single_sequence_example说到这个方法我们就得提一下tf.train.SequenceExample，望文生义，它与tf.train.Example方法的不同之处在于它多了一个边长的list部分，定义如下 message SequenceExample { Features context = 1; FeatureLists feature_lists = 2;};12我们以源码中电影打分示例来了解一下SequenceExample // context: {// feature: {// key : “locale”// value: {// bytes_list: {// value: [ “pt_BR” ]// }// }// }// feature: {// key : “age”// value: {// float_list: {// value: [ 19.0 ]// }// }// }// feature: {// key : “favorites”// value: {// bytes_list: {// value: [ “Majesty Rose”, “Savannah Outen”, “One Direction” ]// }// }// }// }// feature_lists: {// feature_list: {// key : “movie_ratings”// value: {// feature: {// float_list: {// value: [ 4.5 ]// }// }// feature: {// float_list: {// value: [ 5.0 ]// }// }// }// }// feature_list: {// key : “movie_names”// value: {// feature: {// bytes_list: {// value: [ “The Shawshank Redemption” ]// }// }// feature: {// bytes_list: {// value: [ “Fight Club” ]// }// }// }// }// feature_list: {// key : “actors”// value: {// feature: {// bytes_list: {// value: [ “Tim Robbins”, “Morgan Freeman” ]// }// }// feature: {// bytes_list: {// value: [ “Brad Pitt”, “Edward Norton”, “Helena Bonham Carter” ]// }// }// }// }// }//1234这是一个用户的电影打分记录，context部分是这个用户不随时间变化的固定属性，而feature_list中就是他观看过的电影，电影的演员，以及对电影打分的记录，是可以一直记录下去的。官方总结的原则如下： // Context:// - All conformant context features K must obey the same conventions as// a conformant Example’s features (see above).// Feature lists:// - A FeatureList L may be missing in an example; it is up to the// parser configuration to determine if this is allowed or considered// an empty list (zero length).// - If a FeatureList L exists, it may be empty (zero length).// - If a FeatureList L is non-empty, all features within the FeatureList// must have the same data type T. Even across SequenceExamples, the type T// of the FeatureList identified by the same key must be the same. An entry// without any values may serve as an empty feature.// - If a FeatureList L is non-empty, it is up to the parser configuration// to determine if all features within the FeatureList must// have the same size. The same holds for this FeatureList across multiple// examples.``` 没有什么复杂的，只是讨论了数据缺失的可能性，另外需要注意list的长度是否固定取决于解析时的设置，例如使用FixedLenSequenceFeature时就会要求等长。 而tf.parse_single_sequence_example就是解析此种数据格式的，与parse_example大同小异，这里就不展开了，也可以从参考文献中找到示例代码 参考资料]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>tfRecord</tag>
        <tag>tf.contrib.learn.run_n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql中对多个字段进行去重计数]]></title>
    <url>%2F2017%2F11%2F30%2Fmysql%E4%B8%AD%E5%AF%B9%E5%A4%9A%E4%B8%AA%E5%AD%97%E6%AE%B5%E8%BF%9B%E8%A1%8C%E5%8E%BB%E9%87%8D%E8%AE%A1%E6%95%B0%2F</url>
    <content type="text"><![CDATA[今天遇到了一个奇怪的bug，举例如下，假设有一张表table colA colB colC a1 b1 NULL a2 b2 NULL a3 b3 NULL 当我们使用SELECT COUNT(DISTINCT colA,colB,colC) FROM talbe进行查询时，返回的竟然是0。但是我们使用SELECT DISTINCT colA,colB,colC FROM talbe可以正常获取三行数据，后来在Stack Overflow上看到SQL_Server中count只支持三种格式 123COUNT(*)COUNT(colName)COUNT(DISTINCT colName) 而查询mysql相关文档没看到这种说法，官方文档中只有count(*)的示例，所以只能认为也是相同的，最终将查询代码改为 1SELECT COUNT(*) FROM (SELECT DISTINCT colA,colB,colC FROM talbe) A 成功的解决了问题，所以在多维度去重的情况下要特别注意，使用正确的语法。]]></content>
      <categories>
        <category>sql相关</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>count distinct</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo设置持续集成过程中遇到的一些问题]]></title>
    <url>%2F2017%2F11%2F26%2Fhexo%E8%AE%BE%E7%BD%AE%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[由于hexo d的方式来布置hexo博客不能满足多台计算机编辑以及备份的需求，所以参考Hexo的版本控制与持续集成的博客，进行了持续集成的配置，效果显著， 终于可以随时随地的编辑博客了。 但是这个过程中也遇到一个问题，就是当我们使用第三方主题的时候（我使用的是next）主题文件夹不能上传到source repository，原因也很简单，因为我是直接使用git clone https://github.com/iissnan/hexo-theme-next themes/next克隆的next主题，所以next文件夹下有.git，是一个独立的repo，当然无法上传，删掉.git后就一切正常了，但这也带来另一个问题，就是我们想更新主题的时候比较麻烦，因为已经是一个独立的本地文件夹了，后来经过搜索，确定了使用git subtree来解决这个问题。 关于git subtree的内容大家可以自行网上搜索，简单来说就是git用来管理子项目的一种方式，整个操作过程如下 12345678910# 添加next仓库，命名为nextgit remote add next https://github.com/iissnan/hexo-theme-next#添加next的master分支到子树git subtree add --prefix=themes/next next master --squash#看看有哪些稳定版本git ls-remote --tags next#测试切换到5.1.2版本git subtree pull --prefix=themes/next next tags/v5.1.2 --squash#测试切换到5.1.3版本git subtree pull --prefix=themes/next next tags/v5.1.3 --squash 在切换到5.1.2版本后，我修改了主题配置文件，添加了livere_uid(來必力评论配置，在next主题下配置这一行就可以开启，简单易配置，不翻墙就可以评论，推荐大家使用)，然后尝试切换到5.1.3版本，发现很智能保留了这个配置（两个版本的配置文件是不同的，merge的过程很顺利），所以通过这个方案切换新版本应该是可行的。 这里需要注意的即使代码的第4行，我一开始写的是git subtree add --prefix=themes/next next tags/v5.1.2 --squash，在这样的情况下5.1.2版本的下载倒是没有问题，但是更新到5.1.3时，会提示已经是最新的代码无法更新，相信大多数人还是以稳定版为主，如果不是想折腾一下源码的话，所以推荐按照现在的顺序来配置。 更新： 最近持续集成过程中出错，提示node版本太低，参考了这边博文, 在appveyor.xml配置文件中新增 1234# Get the latest stable version of Node.js or io.js- ps: Install-Product node $env:nodejs_version# install modules- npm install 也即node更新语句后解决]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>CI</tag>
        <tag>git subtree</tag>
        <tag>來必力</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yii1.1版本下访问数据库时需要注意的问题]]></title>
    <url>%2F2017%2F11%2F26%2Fyii1-1%E7%89%88%E6%9C%AC%E4%B8%8B%E8%AE%BF%E9%97%AE%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%B6%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[上篇文章提到我们使用了greenplum来代替mysql来适应数据规模较大的情况，同时尽量保证小的改动。服务端我们使用的是yii1.1版本，之所以不是2.0原因很简单，历史选择。但在使用greenplum的过程中我们发现某些查询下速度特别慢，后经过各种测试发现，一旦调用了prepare方法，在数据量较大时速度就会特别慢，怀疑时greenplum优化的问题，但由于使用的是dba自己改造后的版本，也就没有深究其官方版本是不是也存在这样的bug。 那么现在的问题就变成了我们使用的是CDbcommand模块queryAll方法，并没有调用其prepare方法，那么为什么还会慢的，阅读其源码发现，所有的query方法，最终都是靠queryInternal方法实现的，在该函数的32行我们不难发现，默认情况先总会调用prepare函数，所以即使我们没有显示的调用CDbcommand的prepare方法，最终还是难以避免这一点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465private function queryInternal($method,$mode,$params=array()) &#123; $params=array_merge($this-&gt;params,$params); if($this-&gt;_connection-&gt;enableParamLogging &amp;&amp; ($pars=array_merge($this-&gt;_paramLog,$params))!==array()) &#123; $p=array(); foreach($pars as $name=&gt;$value) $p[$name]=$name.'='.var_export($value,true); $par='. Bound with '.implode(', ',$p); &#125; else $par=''; Yii::trace('Querying SQL: '.$this-&gt;getText().$par,'system.db.CDbCommand'); if($this-&gt;_connection-&gt;queryCachingCount&gt;0 &amp;&amp; $method!=='' &amp;&amp; $this-&gt;_connection-&gt;queryCachingDuration&gt;0 &amp;&amp; $this-&gt;_connection-&gt;queryCacheID!==false &amp;&amp; ($cache=Yii::app()-&gt;getComponent($this-&gt;_connection-&gt;queryCacheID))!==null) &#123; $this-&gt;_connection-&gt;queryCachingCount--; $cacheKey='yii:dbquery'.':'.$method.':'.$this-&gt;_connection-&gt;connectionString.':'.$this-&gt;_connection-&gt;username; $cacheKey.=':'.$this-&gt;getText().':'.serialize(array_merge($this-&gt;_paramLog,$params)); if(($result=$cache-&gt;get($cacheKey))!==false) &#123; Yii::trace('Query result found in cache','system.db.CDbCommand'); return $result[0]; &#125; &#125; try &#123; if($this-&gt;_connection-&gt;enableProfiling) Yii::beginProfile('system.db.CDbCommand.query('.$this-&gt;getText().$par.')','system.db.CDbCommand.query'); $this-&gt;prepare(); if($params===array()) $this-&gt;_statement-&gt;execute(); else $this-&gt;_statement-&gt;execute($params); if($method==='') $result=new CDbDataReader($this); else &#123; $mode=(array)$mode; call_user_func_array(array($this-&gt;_statement, 'setFetchMode'), $mode); $result=$this-&gt;_statement-&gt;$method(); $this-&gt;_statement-&gt;closeCursor(); &#125; if($this-&gt;_connection-&gt;enableProfiling) Yii::endProfile('system.db.CDbCommand.query('.$this-&gt;getText().$par.')','system.db.CDbCommand.query'); if(isset($cache,$cacheKey)) $cache-&gt;set($cacheKey, array($result), $this-&gt;_connection-&gt;queryCachingDuration, $this-&gt;_connection-&gt;queryCachingDependency); return $result; &#125; catch(Exception $e) &#123; if($this-&gt;_connection-&gt;enableProfiling) Yii::endProfile('system.db.CDbCommand.query('.$this-&gt;getText().$par.')','system.db.CDbCommand.query'); $errorInfo=$e instanceof PDOException ? $e-&gt;errorInfo : null; $message=$e-&gt;getMessage(); Yii::log(Yii::t('yii','CDbCommand::&#123;method&#125;() failed: &#123;error&#125;. The SQL statement executed was: &#123;sql&#125;.', array('&#123;method&#125;'=&gt;$method, '&#123;error&#125;'=&gt;$message, '&#123;sql&#125;'=&gt;$this-&gt;getText().$par)),CLogger::LEVEL_ERROR,'system.db.CDbCommand'); if(YII_DEBUG) $message.='. The SQL statement executed was: '.$this-&gt;getText().$par; throw new CDbException(Yii::t('yii','CDbCommand failed to execute the SQL statement: &#123;error&#125;', array('&#123;error&#125;'=&gt;$message)),(int)$e-&gt;getCode(),$errorInfo); &#125; &#125; 最终的解决方案是在配置中加了’emulatePrepare’ =&gt; true的配置，该配置项其实就是启用了pdo中的模拟预处理，用模拟预处理代替greenplum龟速的预处理，最终解决了该问题。]]></content>
      <categories>
        <category>web开发</category>
        <category>数据库访问</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
        <tag>yii</tag>
        <tag>emulate prepare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql与postgresql的区别]]></title>
    <url>%2F2017%2F11%2F14%2Fmysql%E4%B8%8Epostgresql%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[在工作中由于数据量太大时mysql负载的压力，我们引入了greenplum做我们大数据量表格的数据库，选择greenplum的原因是它在解决大数据存储的同时，尽可能的保持了和mysql一样的操作逻辑（同是关系型数据库，使用sql查询），减小了我们的迁移成本，但是尽管如此，greenplum基于的postgresql引擎和mysql还是存在一定的差异，此文对目前遇到的问题进行了记录，以防止以后再次出现同样的问题。 反引号的区别 在mysql中，对于保留字使用反引号来加以区分，例如想查询一个表中的select字段，由于select是保留字我们需写成 SELECT `select` FROM table 而在postgresql中不存在这种写法，当identifier为保留字时，则使用双引号 SELECT &quot;select&quot; FROM table limit的差别 在mysql中limit可以间简写为limit 10,100，其等价于limit 100 offset 10而在postgresql中只能采用第二种标准的sql方式 对if语句的支持 mysql支持if 语句，例如if(x&gt;1,1,0)而postgresql只支持case when语句 大小写 mysql中是不区分大小写的，你查询一个字段，写大小写都可以，而在postgresql中是区分大小写的，如果想要字段名大些，必须用双引号将其扩起，由于一些历史原因mysql中使用了大写的字段名，查询也没有问题，但是到postgresql中就失败了，所以建议在sql中，采用下划线分割，不要使用驼峰命名发 以上就是目前遇到的mysql迁移greenplum过程中遇到的一些问题，可以看到大部分是因为mysql在标准sql的基础上给我们提供了更加方便的解决方式，但这种方式在迁移数据库时会给我们带来意想不到的麻烦，所以除非肯定只用一种数据库，否则还是建议按照标准sql书写]]></content>
      <categories>
        <category>sql相关</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>postgresql</tag>
        <tag>greenplum</tag>
      </tags>
  </entry>
</search>
