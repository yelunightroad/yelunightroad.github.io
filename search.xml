<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用训练好的word2vec词典时需要注意的问题]]></title>
    <url>%2F2019%2F01%2F16%2F%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84word2vec%E8%AF%8D%E5%85%B8%E6%97%B6%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在模型中使用训练好的word2vec向量时，我们通常使用一下代码 12self.reader = tf.train.NewCheckpointReader(self.embedding_cpt_path)self.embedding_W = self.reader.get_tensor('w_in') #w_in是训练好的词向量变量 但是当该词向量字典过大时，我们就会遇到以下错误 1ValueError: Cannot create a tensor proto whose content is larger than 2GB 这是因为除变量外，像常量等很多都不允许超过2GB（因为要序列化到pb），所以当你要加载一个2GB以上的数据时，将它声明为变量。 12345self.embedding_W = tf.Variable(tf.constant(0.0, shape=[word_size, embedding_size]), trainable=False, name="e_w")self.embedding_saver = tf.train.Saver(&#123;"w_in": self.embedding_W&#125;)sess = tf.Session()self.embedding_saver.restore(sess, "checkpoint_filename.ckpt") #在全局初始化后进行 参考资料 https://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow https://stackoverflow.com/questions/51244489/valueerror-cannot-create-a-tensor-proto-whose-content-is-larger-than-2gb]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>word2vec</tag>
        <tag>larger than 2GB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[永远不要使用array_merge]]></title>
    <url>%2F2018%2F12%2F12%2F%E6%B0%B8%E8%BF%9C%E4%B8%8D%E8%A6%81%E4%BD%BF%E7%94%A8array-merge%2F</url>
    <content type="text"><![CDATA[array_merge是php中的一个常用函数，我在之前也经常使用这个函数，直到有一天我发现我的程序运行的比预想中要慢的多，经过逐行排查发现竟然是array_merge函数惹得祸 123456$primes = array();while (xxx)&#123; $data = get_some_data(); $primes = array_merge($primes, $data);&#125; 当你使用类似上面的函数去不断的合并数组，尤其是最终结果较大时你会发现时间变得不可忍受。 解决的方法也很简单 12345678$primes = array();while (xxx)&#123; $data = get_some_data(); foreach($data as $val) &#123; $primes[] = $val; &#125;&#125; 差别大到你无法想象。]]></content>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysqlpp避坑指南]]></title>
    <url>%2F2018%2F11%2F27%2Fmysqlpp%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[开发线上服务的过程中避免不了使用到数据库，由于使用了c++进行开发，在连接mysql的过程中我们使用了mysql++来进行链接，但是在使用过程中遇到了很多意想不到的情况，我们记录如下 mysql服务器版本：5.6.36 mysql++版本：3.2.4 问题的核心在于作为一个线上服务，我们显然希望使用长连接来增加效率，所以第一步我们需要调整数据库的连接关闭时间，默认为8小时，通常是够用的，但是如果使用公司dba提供的实例的话需要额外注意这个时间，最好调整到一小时以上。 1wait_timeout=3600 即使是调整了这个时间，我们也无法保证在时限内一定有用户访问，所以当连接断开时我们一定要考虑重连的问题，经过查询我们找到了最简单的解决问题的方案。 12mysqlpp::Connection m_con_;m_con_.set_option( new mysqlpp::ReconnectOption(true)); 然而理想很丰满，显示很骨感，该设置无效，在网上也有想过的案例，所以改用其它方法。 12m_con_.connected()m_con_.ping() 以上两个方法都是判断数据库连接是否可用，ping()方法比connected()方法更加可靠，因为connected()只是判断你是否进行过成功的connect()操作，至于后来是否还可用就不是它的职责了，理论上我们只需要在请求mysql之前先判断一下，然后如果断开连接进行重连就好了。然而事实是进行了connect之后第一次调用connected方法返回true，但是第二次开始就返回false了，我们只好把希望放在ping()方法上，然而查看mysql++源码得知ping()方法会调用connected()方法，所以这条路走不通了。 最后我们采用了以下方法 123456789101112131415161718192021bool CMysqlClient::Reconnect() &#123; try &#123; m_con_.disconnect(); m_con_.set_option(new mysqlpp::SetCharsetNameOption("utf8")); if (!m_con_.connect(m_mysql_dbname_.c_str(), m_mysql_dbhost_.c_str(), m_mysql_dbuser_.c_str(), m_mysql_dbpass_.c_str() )) &#123; return false; &#125; &#125; catch ( const mysqlpp::Exception&amp; e ) &#123; return false; &#125; return true;&#125;for (int i = 0; i &lt; 3; i++) &#123; try&#123; //mysql查询 &#125; catch(const mysqlpp::BadQuery&amp; bq) &#123; Reconnect(); &#125;&#125; 当链接丢失时，进行mysql查询会抛出mysqlpp::BadQuery异常，这是进行重连即可。 注意 代码的第三行，在进行重连之前我们进行了disconnect()操作，理论上异常的时候连接已经丢失，不需要该操作，但是实际使用中我们发现，没用改语句的情况下，在有些机器上没问题，但是在有些机器上第4行的set_option()命令会抛出异常，所以我们加入了第3行的操作。 以上就是我们在使用mysql++访问数据库是遇到的所有问题。]]></content>
      <tags>
        <tag>mysql</tag>
        <tag>长连接</tag>
        <tag>mysql++</tag>
        <tag>mysqlpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法小结]]></title>
    <url>%2F2018%2F07%2F13%2FEM%E7%AE%97%E6%B3%95%E6%91%98%E8%A6%81%2F</url>
    <content type="text"><![CDATA[EM算法是机器学习领域常见的一种算法了，由于最近又多次看到跟其相关的东西，所以做一个小结以备忘。 个人认为EM中最终的是理解参数和隐状态这两个点。此处引用李航《统计学习方法》中的例子来做说明。 假设有三枚硬币A、B、C，这些硬币正面出现的概率分别是 ππ 、pp 、qq 。进行如下掷硬币试验： 先掷A，如果A是正面则再掷B，如果A是反面则再掷C。对于B或C的结果，如果是正面则记为1，如果是反面则记为0。进行 NN 次独立重复实验，得到结果。现在只能观测到结果，不能观测到掷硬币的过程，估计模型参数 θ=(π,p,q) 。 在这个问题中，实验结果是可观测数据$ Y=(y_1,…,y_N)$，硬币A的结果是不可观测数据 $Z=(z_1,…,z_N)$且 z 只有两种可能取值1和0。 我们需要求得$\hat{\theta}=arg max{\theta}{logP(Y|\theta)}$这个问题没有解析解，只能迭代求解，因为$p(y|\theta)=\sum{Z}P(Z|\theta)P(Y|Z,\theta)$需要依赖于隐状态。所以就想出了一种迭代求值的方式 先假设一个初始参数$\theta^0$ E步被称为期望步，通俗点说就是利用上一步的参数$\theta^{i-1}$，以此参数求出隐状态的条件概率概率，严谨一点定义的话我们需要传说中的Q函数：完全数据的对数似然函数（观测数据和隐变量数据构成完全数据）关于在给定观测数据Y和当前参数$\theta^{i-1}$下对未观测数据Z的条件概率分布的期望。公式如下：$$\begin{aligned}Q(\theta,\theta^{(i)})&amp;=EZ[logP(Y,Z|\theta)]\&amp;=\sum{Z}logP(Y,Z|\theta)P(Z|Y,\theta^{(i-1)})\end{aligned}\tag{1}\label{1}$$这里，$P(Z|Y,\theta^{(i-1)})$是在给定观测数据Y和上一轮参数估计$\theta^{(i-1)}$下隐变量数据Z的条件概率分布，也就是我们E步求得最重要的一个值，三硬币模型中就是求得$y_j$来自硬币B的概率（已知y和$\theta$求得硬币A证明朝上的概率） M步：求使$Q(\theta,\theta^{(i)})$极大化的$\theta$，更新$\theta^{(i)}=arg \max_{\theta}Q(\theta,\theta^{(i-1)})$，在三硬币模型中也就是知道每次硬币来自硬币B的概率$\mui$的情况下更新参数$\theta$，二项分布最优值为$\frac{k}{n}$时，$\pi^{(i)}=\frac{1}{n}\sum{j=1}^{n}\muj^{i}$，$p^{(i)}=\frac{\sum{j=1}^{n}\mu_j^{i}yi}{\sum{j=1}^{n}\mu_j^{i}}$ 至于为什么不断迭代Q函数就可以获取有隐状态下的最大似然，用Jensen不等式证明，这里不展开。 再通过几个例子加强下 例1:现在一个班里有100个男生，100个女生。我们假定男生的身高服从正态分布 ，女生的身高则服从另一个正态分布： 。假如男女是分开的，这时候我们可以用极大似然法（MLE），分别通过这100个男生和100个女生的样本来估计这两个正态分布的参数。但现在情况要更复杂一点，就是这100个男生和100个女生混在一起了。我们拥有100个人的身高数据，却不知道这100个人每一个是男生还是女生。 那么我们一个样本是男生还是女生就是一个隐状态。 E步，我们利用先验知识之类的先假设出男生和女生的正态分布参数（或者来自于迭代的上一步），这样我们就可以估算出一个样本是男生还是女生 M步，我们区分出了每个样本是男生还是女生，就可以更新正态分布参数 例2：对于常见的k-means算法，我们认为每个点属于哪个模型是隐状态，聚类中心点是参数 E步，假设一些聚类中心点，求取隐状态每个点的类别 M步，根据隐状态更新中心点 总结理解EM算法最重要的个人认为是找到隐状态和参数，E步根据参数求取隐状态，M步根据隐状态求取是Q函数最大的参数。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow中的axis理解]]></title>
    <url>%2F2018%2F04%2F11%2FTensorFlow%E4%B8%AD%E7%9A%84axis%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[在TensorFlow的很多函数中都有到了axis参数，对于这个参数怎么理解容易让人犯晕，二维情况下还好，如果升到高维更加难以理解，所以本文用三维情况下的两个常见函数为例，让我们彻底理解这个参数的意义。 在开始之前，先开宗明义，axis参数的意义就是在哪个维度上做操作 tf.reduce_max()在介绍这个函数之前，我们先简单说一下reduce的含义，这个操作无论在python或者是更高级的mapreduce中都会碰到，我对它的解释就是对列表中的元素做某一连续操作。例如reduce_max就是做连续的max操作，例如对列表[1,2,3,4]进行reduce_max就是先对元素1和2进行max操作，然后其结果和3再做max，再然后结果和4做max。理解了reduce操作后我们自然就明白了axis的必要性，如果把一个多维数组简单的当做一个一维列表的话进行reduce操作毫无疑问是没问题的，事实上如果不加axis参数的话，tf.reduce_max就是这么干的。 例如如下代码 1234import tensorflow as tfa=[[[1,2,3,4],[5,6,7,8],[8,9,10,11]],[[11,12,13,14],[15,16,17,18],[19,20,21,22]]]sess = tf.InteractiveSession()print tf.reduce_max(a).eval() 运行结果为 122 但是有时候我们不希望这么简单粗暴，毕竟如果这么计算的话我们定义这么复杂的结构就毫无意义了，例如我们有一个二维数组 12s=[[14,175], [13,182]] 代表了我们班里有两个同学，小明14岁身高175，小刚13岁身高182，我们想通过一个运算获取班里最大的同学多少岁，以及最高的同学多高，这个时候axis就排上了用场，用上面的例子来说，axis为0表示同学维度，axis为1表示同学自身属性维度。 12345 ___属性__| 学[[14,175],生[13,182]]| 在维度0运行reduce操作表示在学生维度上进行迭代，最终学生维度小时，两个属性维度保留，我们得到了一个‘超级学生’，岁数最大，身高最高 1print tf.reduce_max(s,0).eval() #output: [ 14 182] 在维度1上运行reduce操作表示在属性维度上进行迭代，我们依然保留了两个‘学生’，只不过他的属性只剩下了最突出（max）的一个 1print tf.reduce_max(s,0).eval() #output: [ 175 182] 虽然两个运算后都只留下了1维，但是一维中的两个元素的含义是不同的。 有了上面的简单示例，我们再用一个稍复杂的例子加深印象 123456import tensorflow as tfa=[[[1,2,3,4],[5,6,7,8],[8,9,10,11]],[[11,12,13,14],[15,16,17,18],[19,20,21,22]]]sess = tf.InteractiveSession()print tf.reduce_max(a,0).eval()print tf.reduce_max(a,1).eval()print tf.reduce_max(a,2).eval() 输出结果为 1234567[[11 12 13 14] [15 16 17 18] [19 20 21 22]][[ 8 9 10 11] [19 20 21 22]][[ 4 8 11] [14 18 22]] 这是一个[2,3,4]维的数组，怎么找每一维呢，很简单，一个中括号代表一维，最外侧的中括号是第0维，我们可以看成 1234567a=[A1，B1] #第0维A=[[1,2,3,4],[5,6,7,8],[8,9,10,11]]B=[[11,12,13,14],[15,16,17,18],[19,20,21,22]]A=[AA,AB,AC]AA=[1,2,3,4]AB=[5,6,7,8]AC=[19,20,21,22] 对第0维操作，就是求A和B较大值（按对应元素求），所以得到一个[3,4]维度的数组 第1维也很明显了，就是A1和B1，那么对第1维操作时，第0维不动，仍然是A，B，但是对A、B本身要进行操作(这里就跟前面的二维数组类似了)，以A为例，要求AA，AB，AC的最大值（按对应元素求），所以A变为了[ 8 9 10 11]，B变为了[19 20 21 22] 第2维留给读者练习 tf.concat()与reduce不同，reduce可以看成是对某一维的缩减，但是concat作为一个连接操作，则是对某一维的扩充。 请看如下示例 12345678import tensorflow as tfa=[[14,175], [13,182]]b=[[14,173], [15,180]]sess = tf.InteractiveSession()print tf.concat([a,b],0).eval() #可以看成两班学生合并，因为是在学生维度合并print tf.concat([a,b],1).eval() 输出结果 123456[[ 14 175] [ 13 182] [ 14 173] [ 15 180]][[ 14 175 14 173] [ 13 182 15 180]] 按照第0维连接，则[2,2]变成了[4,2]，也即可以看成 123a=[A,B]b=[C,D]#连接后是[A,B,C,D] 按照第1维连接，要保证0维还是两个元素，然后要对A进行操作，和C合并。 总结axis的意义就是你选择的操作在某一维度上执行，执行后的结果与你选择的操作有关，你选择reduce操作后，所选择维度小时，你选择concat操作后所选择维度进行连接，元素扩展，多tensor操作时按照对应位置进行操作，如concat示例中A对应C，选择维度直观上说就是看第几层括号，以最后一个示例为例，维度0就是最外层括号内元素变化，维度1就是里层括号A=[14,175]合并了C=[13,173]。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>tf.reduce_max</tag>
        <tag>tf.concat，axis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow中tfRecord的使用以及测试]]></title>
    <url>%2F2018%2F04%2F09%2Ftensorflow%E4%B8%ADtfRecord%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BB%A5%E5%8F%8A%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[tfRecord是TensorFlow官方推荐的标准数据格式，对于存储字典类型的数据比较友好，所以撰写此文记录一下使用过程中的一点经验。 tfRecord结构tfRecord中存储的记录由tf.train.Example定义，其实就是一个protocol buffer文件，定义如下： 123456789101112131415message Example &#123; Features features = 1;&#125;;message Features&#123; map&lt;string,Feature&gt; featrue = 1;&#125;;message Feature&#123; oneof kind&#123; BytesList bytes_list = 1; FloatList float_list = 2; Int64List int64_list = 3; &#125;&#125;; 可以看到其主要包含了一个字符串格式的key，以及字符串、浮点数或者整数类型的列表构成的value 代码示例12345678import tensorflow as tfimport oskeys=[[1.0,2.0],[2.0,3.0],[1,2]]keys2=[[1.0,2.0],[2.0,3.0],[3.0]]strtest='aaa'sess=tf.InteractiveSession()sess.run(tf.global_variables_initializer()) 基础变量，没什么可解释的 1234567891011121314151617181920212223def make_example(key): example = tf.train.Example(features=tf.train.Features( feature=&#123; 'ft':tf.train.Feature(float_list=tf.train.FloatList(value=key)), 'st':tf.train.Feature(bytes_list=tf.train.BytesList(value=[strtest])) &#125; )) return examplefilename="tmp.tfrecords"if os.path.exists(filename): os.remove(filename)writer = tf.python_io.TFRecordWriter(filename)for key in keys: ex = make_example(key) writer.write(ex.SerializeToString())writer.close()reader = tf.TFRecordReader()filename_queue = tf.train.string_input_producer(["tmp.tfrecords"],num_epochs=2)_,serialized_example =reader.read(filename_queue)batch = tf.train.batch(tensors=[serialized_example],batch_size=3) 在以上代码中我们展示了如何定义一个example并将其写入文件，同事19行开始我们也对如何读取文件做了简单示例，现在读取部分也可以使用dataset，更加符合官方潮流，例如用一下代码代替19-23行 12dataset = tf.data.TFRecordDataset(filenames=filenames, buffer_size=200).batch(3)record_iter = dataset.make_one_shot_iterator() 下面进入重头戏，如何解析tfRecord的内容 12345678910features=&#123; "ft":tf.VarLenFeature(tf.float32), "st":tf.VarLenFeature(tf.string)&#125;key_parsed = tf.parse_example(batch,features)sess.run(tf.initialize_local_variables())print tf.contrib.learn.run_n(key_parsed) #tf.train.get_or_create_global_step()#print tf.contrib.training.train(key_parsed,'../') 输出如下 12345678[&#123;&apos;ft&apos;: SparseTensorValue(indices=array([[0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]]), values=array([1., 2., 2., 3., 1., 2.], dtype=float32), dense_shape=array([3, 2])), &apos;st&apos;: SparseTensorValue(indices=array([[0, 0], [1, 0], [2, 0]]), values=array([&apos;aaa&apos;, &apos;aaa&apos;, &apos;aaa&apos;], dtype=object), dense_shape=array([3, 1]))&#125;] 其实非常简单，定义一个同样的feature，然后用相应的方法获取就可以了，注意代码的第8行的方法已经废弃了，TensorFlow相关人员表示没有开发其替代函数的打算，想实现相应的效果可以使用第9、10行的代码，基本可以替代，但是做实验的话由于train方法其实是在训练，还是8行的函数简单些，在删除前我们暂且继续使用。 在以上代码中我们使用的是VarlenFeature，它更通用一些，另外比较常见的还有FixedLenFeature，使用代码如下 1234567features=&#123; &quot;ft&quot;:tf.FixedLenFeature(shape=[2],dtype=tf.float32,default_value=[2.0,3.0])&#125;key_parsed = tf.parse_example(batch,features)print tf.contrib.learn.run_n(key_parsed) 输出结果为 123[&#123;&apos;ft&apos;: array([[1., 2.], [2., 3.], [1., 2.]], dtype=float32)&#125;] 这里需要注意的是它解析的数据必须是定长的，且数据格式与shape参数必须一致，否则就会报错。另外它返回的是一个tensor不同于VarLenFeature返回的是sparseTensor。这里default_value的意义是你查询的字段不存在时赋予默认值，比如你现在查询的是ft字段，如果存储时只存储了st那么就会使用默认值赋值，所以default_value，的shape必须与shape参数一致。 有了以上两个参数基本可以解决我们的数据读写问题，如果还不够，可以了解一下SparseFeature，它可以以一种类似SparseTensor的方式帮你定义数据 123456789101112131415161718192021222324`serialized`: [ features &#123; feature &#123; key: &quot;val&quot; value &#123; float_list &#123; value: [ 0.5, -1.0 ] &#125; &#125; &#125; feature &#123; key: &quot;ix&quot; value &#123; int64_list &#123; value: [ 3, 20 ] &#125; &#125; &#125; &#125;, features &#123; feature &#123; key: &quot;val&quot; value &#123; float_list &#123; value: [ 0.0 ] &#125; &#125; &#125; feature &#123; key: &quot;ix&quot; value &#123; int64_list &#123; value: [ 42 ] &#125; &#125; &#125; &#125; ] #And arguments example_names: [&quot;input0&quot;, &quot;input1&quot;], features: &#123; &quot;sparse&quot;: SparseFeature( index_key=&quot;ix&quot;, value_key=&quot;val&quot;, dtype=tf.float32, size=100), &#125; #Then the output is a dictionary: &#123; &quot;sparse&quot;: SparseTensor( indices=[[0, 3], [0, 20], [1, 42]], values=[0.5, -1.0, 0.0] dense_shape=[2, 100]), &#125; 其它相关函数除了以上用到的parse_example方法，还有一些其它方法我们可以简单了解。 tf.parse_single_example该方法与tf.parse_example方法基本相同，只不过少了batch函数，单独解析 tf.parse_single_sequence_example说到这个方法我们就得提一下tf.train.SequenceExample，望文生义，它与tf.train.Example方法的不同之处在于它多了一个边长的list部分，定义如下 1234message SequenceExample &#123; Features context = 1; FeatureLists feature_lists = 2;&#125;; 我们以源码中电影打分示例来了解一下SequenceExample 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// context: &#123;// feature: &#123;// key : &quot;locale&quot;// value: &#123;// bytes_list: &#123;// value: [ &quot;pt_BR&quot; ]// &#125;// &#125;// &#125;// feature: &#123;// key : &quot;age&quot;// value: &#123;// float_list: &#123;// value: [ 19.0 ]// &#125;// &#125;// &#125;// feature: &#123;// key : &quot;favorites&quot;// value: &#123;// bytes_list: &#123;// value: [ &quot;Majesty Rose&quot;, &quot;Savannah Outen&quot;, &quot;One Direction&quot; ]// &#125;// &#125;// &#125;// &#125;// feature_lists: &#123;// feature_list: &#123;// key : &quot;movie_ratings&quot;// value: &#123;// feature: &#123;// float_list: &#123;// value: [ 4.5 ]// &#125;// &#125;// feature: &#123;// float_list: &#123;// value: [ 5.0 ]// &#125;// &#125;// &#125;// &#125;// feature_list: &#123;// key : &quot;movie_names&quot;// value: &#123;// feature: &#123;// bytes_list: &#123;// value: [ &quot;The Shawshank Redemption&quot; ]// &#125;// &#125;// feature: &#123;// bytes_list: &#123;// value: [ &quot;Fight Club&quot; ]// &#125;// &#125;// &#125;// &#125;// feature_list: &#123;// key : &quot;actors&quot;// value: &#123;// feature: &#123;// bytes_list: &#123;// value: [ &quot;Tim Robbins&quot;, &quot;Morgan Freeman&quot; ]// &#125;// &#125;// feature: &#123;// bytes_list: &#123;// value: [ &quot;Brad Pitt&quot;, &quot;Edward Norton&quot;, &quot;Helena Bonham Carter&quot; ]// &#125;// &#125;// &#125;// &#125;// &#125;// 这是一个用户的电影打分记录，context部分是这个用户不随时间变化的固定属性，而feature_list中就是他观看过的电影，电影的演员，以及对电影打分的记录，是可以一直记录下去的。 官方总结的原则如下： 12345678910111213141516// Context:// - All conformant context features K must obey the same conventions as// a conformant Example&apos;s features (see above).// Feature lists:// - A FeatureList L may be missing in an example; it is up to the// parser configuration to determine if this is allowed or considered// an empty list (zero length).// - If a FeatureList L exists, it may be empty (zero length).// - If a FeatureList L is non-empty, all features within the FeatureList// must have the same data type T. Even across SequenceExamples, the type T// of the FeatureList identified by the same key must be the same. An entry// without any values may serve as an empty feature.// - If a FeatureList L is non-empty, it is up to the parser configuration// to determine if all features within the FeatureList must// have the same size. The same holds for this FeatureList across multiple// examples. 没有什么复杂的，只是讨论了数据缺失的可能性，另外需要注意list的长度是否固定取决于解析时的设置，例如使用FixedLenSequenceFeature时就会要求等长。 而tf.parse_single_sequence_example就是解析此种数据格式的，与parse_example大同小异，这里就不展开了，也可以从参考资料中找到示例代码 参考资料Tensorflow高阶读写教程 TensorFlow源码]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>tfRecord</tag>
        <tag>tf.contrib.learn.run_n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql中对多个字段进行去重计数]]></title>
    <url>%2F2017%2F11%2F30%2Fmysql%E4%B8%AD%E5%AF%B9%E5%A4%9A%E4%B8%AA%E5%AD%97%E6%AE%B5%E8%BF%9B%E8%A1%8C%E5%8E%BB%E9%87%8D%E8%AE%A1%E6%95%B0%2F</url>
    <content type="text"><![CDATA[今天遇到了一个奇怪的bug，举例如下，假设有一张表table colA colB colC a1 b1 NULL a2 b2 NULL a3 b3 NULL 当我们使用SELECT COUNT(DISTINCT colA,colB,colC) FROM talbe进行查询时，返回的竟然是0。但是我们使用SELECT DISTINCT colA,colB,colC FROM talbe可以正常获取三行数据，后来在Stack Overflow上看到SQL_Server中count只支持三种格式 123COUNT(*)COUNT(colName)COUNT(DISTINCT colName) 而查询mysql相关文档没看到这种说法，官方文档中只有count(*)的示例，所以只能认为也是相同的，最终将查询代码改为 1SELECT COUNT(*) FROM (SELECT DISTINCT colA,colB,colC FROM talbe) A 成功的解决了问题，所以在多维度去重的情况下要特别注意，使用正确的语法。]]></content>
      <categories>
        <category>sql相关</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>count distinct</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo设置持续集成过程中遇到的一些问题]]></title>
    <url>%2F2017%2F11%2F26%2Fhexo%E8%AE%BE%E7%BD%AE%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[由于hexo d的方式来布置hexo博客不能满足多台计算机编辑以及备份的需求，所以参考Hexo的版本控制与持续集成的博客，进行了持续集成的配置，效果显著， 终于可以随时随地的编辑博客了。 但是这个过程中也遇到一个问题，就是当我们使用第三方主题的时候（我使用的是next）主题文件夹不能上传到source repository，原因也很简单，因为我是直接使用git clone https://github.com/iissnan/hexo-theme-next themes/next克隆的next主题，所以next文件夹下有.git，是一个独立的repo，当然无法上传，删掉.git后就一切正常了，但这也带来另一个问题，就是我们想更新主题的时候比较麻烦，因为已经是一个独立的本地文件夹了，后来经过搜索，确定了使用git subtree来解决这个问题。 关于git subtree的内容大家可以自行网上搜索，简单来说就是git用来管理子项目的一种方式，整个操作过程如下 12345678910# 添加next仓库，命名为nextgit remote add next https://github.com/iissnan/hexo-theme-next#添加next的master分支到子树git subtree add --prefix=themes/next next master --squash#看看有哪些稳定版本git ls-remote --tags next#测试切换到5.1.2版本git subtree pull --prefix=themes/next next tags/v5.1.2 --squash#测试切换到5.1.3版本git subtree pull --prefix=themes/next next tags/v5.1.3 --squash 在切换到5.1.2版本后，我修改了主题配置文件，添加了livere_uid(來必力评论配置，在next主题下配置这一行就可以开启，简单易配置，不翻墙就可以评论，推荐大家使用)，然后尝试切换到5.1.3版本，发现很智能保留了这个配置（两个版本的配置文件是不同的，merge的过程很顺利），所以通过这个方案切换新版本应该是可行的。 这里需要注意的即使代码的第4行，我一开始写的是git subtree add --prefix=themes/next next tags/v5.1.2 --squash，在这样的情况下5.1.2版本的下载倒是没有问题，但是更新到5.1.3时，会提示已经是最新的代码无法更新，相信大多数人还是以稳定版为主，如果不是想折腾一下源码的话，所以推荐按照现在的顺序来配置。 更新： 最近持续集成过程中出错，提示node版本太低，参考了这边博文, 在appveyor.xml配置文件中新增 1234# Get the latest stable version of Node.js or io.js- ps: Install-Product node $env:nodejs_version# install modules- npm install 也即node更新语句后解决]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>CI</tag>
        <tag>git subtree</tag>
        <tag>來必力</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yii1.1版本下访问数据库时需要注意的问题]]></title>
    <url>%2F2017%2F11%2F26%2Fyii1-1%E7%89%88%E6%9C%AC%E4%B8%8B%E8%AE%BF%E9%97%AE%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%B6%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[上篇文章提到我们使用了greenplum来代替mysql来适应数据规模较大的情况，同时尽量保证小的改动。服务端我们使用的是yii1.1版本，之所以不是2.0原因很简单，历史选择。但在使用greenplum的过程中我们发现某些查询下速度特别慢，后经过各种测试发现，一旦调用了prepare方法，在数据量较大时速度就会特别慢，怀疑时greenplum优化的问题，但由于使用的是dba自己改造后的版本，也就没有深究其官方版本是不是也存在这样的bug。 那么现在的问题就变成了我们使用的是CDbcommand模块queryAll方法，并没有调用其prepare方法，那么为什么还会慢的，阅读其源码发现，所有的query方法，最终都是靠queryInternal方法实现的，在该函数的32行我们不难发现，默认情况先总会调用prepare函数，所以即使我们没有显示的调用CDbcommand的prepare方法，最终还是难以避免这一点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465private function queryInternal($method,$mode,$params=array()) &#123; $params=array_merge($this-&gt;params,$params); if($this-&gt;_connection-&gt;enableParamLogging &amp;&amp; ($pars=array_merge($this-&gt;_paramLog,$params))!==array()) &#123; $p=array(); foreach($pars as $name=&gt;$value) $p[$name]=$name.'='.var_export($value,true); $par='. Bound with '.implode(', ',$p); &#125; else $par=''; Yii::trace('Querying SQL: '.$this-&gt;getText().$par,'system.db.CDbCommand'); if($this-&gt;_connection-&gt;queryCachingCount&gt;0 &amp;&amp; $method!=='' &amp;&amp; $this-&gt;_connection-&gt;queryCachingDuration&gt;0 &amp;&amp; $this-&gt;_connection-&gt;queryCacheID!==false &amp;&amp; ($cache=Yii::app()-&gt;getComponent($this-&gt;_connection-&gt;queryCacheID))!==null) &#123; $this-&gt;_connection-&gt;queryCachingCount--; $cacheKey='yii:dbquery'.':'.$method.':'.$this-&gt;_connection-&gt;connectionString.':'.$this-&gt;_connection-&gt;username; $cacheKey.=':'.$this-&gt;getText().':'.serialize(array_merge($this-&gt;_paramLog,$params)); if(($result=$cache-&gt;get($cacheKey))!==false) &#123; Yii::trace('Query result found in cache','system.db.CDbCommand'); return $result[0]; &#125; &#125; try &#123; if($this-&gt;_connection-&gt;enableProfiling) Yii::beginProfile('system.db.CDbCommand.query('.$this-&gt;getText().$par.')','system.db.CDbCommand.query'); $this-&gt;prepare(); if($params===array()) $this-&gt;_statement-&gt;execute(); else $this-&gt;_statement-&gt;execute($params); if($method==='') $result=new CDbDataReader($this); else &#123; $mode=(array)$mode; call_user_func_array(array($this-&gt;_statement, 'setFetchMode'), $mode); $result=$this-&gt;_statement-&gt;$method(); $this-&gt;_statement-&gt;closeCursor(); &#125; if($this-&gt;_connection-&gt;enableProfiling) Yii::endProfile('system.db.CDbCommand.query('.$this-&gt;getText().$par.')','system.db.CDbCommand.query'); if(isset($cache,$cacheKey)) $cache-&gt;set($cacheKey, array($result), $this-&gt;_connection-&gt;queryCachingDuration, $this-&gt;_connection-&gt;queryCachingDependency); return $result; &#125; catch(Exception $e) &#123; if($this-&gt;_connection-&gt;enableProfiling) Yii::endProfile('system.db.CDbCommand.query('.$this-&gt;getText().$par.')','system.db.CDbCommand.query'); $errorInfo=$e instanceof PDOException ? $e-&gt;errorInfo : null; $message=$e-&gt;getMessage(); Yii::log(Yii::t('yii','CDbCommand::&#123;method&#125;() failed: &#123;error&#125;. The SQL statement executed was: &#123;sql&#125;.', array('&#123;method&#125;'=&gt;$method, '&#123;error&#125;'=&gt;$message, '&#123;sql&#125;'=&gt;$this-&gt;getText().$par)),CLogger::LEVEL_ERROR,'system.db.CDbCommand'); if(YII_DEBUG) $message.='. The SQL statement executed was: '.$this-&gt;getText().$par; throw new CDbException(Yii::t('yii','CDbCommand failed to execute the SQL statement: &#123;error&#125;', array('&#123;error&#125;'=&gt;$message)),(int)$e-&gt;getCode(),$errorInfo); &#125; &#125; 最终的解决方案是在配置中加了’emulatePrepare’ =&gt; true的配置，该配置项其实就是启用了pdo中的模拟预处理，用模拟预处理代替greenplum龟速的预处理，最终解决了该问题。]]></content>
      <categories>
        <category>web开发</category>
        <category>数据库访问</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
        <tag>yii</tag>
        <tag>emulate prepare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql与postgresql的区别]]></title>
    <url>%2F2017%2F11%2F14%2Fmysql%E4%B8%8Epostgresql%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[在工作中由于数据量太大时mysql负载的压力，我们引入了greenplum做我们大数据量表格的数据库，选择greenplum的原因是它在解决大数据存储的同时，尽可能的保持了和mysql一样的操作逻辑（同是关系型数据库，使用sql查询），减小了我们的迁移成本，但是尽管如此，greenplum基于的postgresql引擎和mysql还是存在一定的差异，此文对目前遇到的问题进行了记录，以防止以后再次出现同样的问题。 反引号的区别 在mysql中，对于保留字使用反引号来加以区分，例如想查询一个表中的select字段，由于select是保留字我们需写成 SELECT `select` FROM table 而在postgresql中不存在这种写法，当identifier为保留字时，则使用双引号 SELECT &quot;select&quot; FROM table limit的差别 在mysql中limit可以间简写为limit 10,100，其等价于limit 100 offset 10而在postgresql中只能采用第二种标准的sql方式 对if语句的支持 mysql支持if 语句，例如if(x&gt;1,1,0)而postgresql只支持case when语句 大小写 mysql中是不区分大小写的，你查询一个字段，写大小写都可以，而在postgresql中是区分大小写的，如果想要字段名大些，必须用双引号将其扩起，由于一些历史原因mysql中使用了大写的字段名，查询也没有问题，但是到postgresql中就失败了，所以建议在sql中，采用下划线分割，不要使用驼峰命名发 以上就是目前遇到的mysql迁移greenplum过程中遇到的一些问题，可以看到大部分是因为mysql在标准sql的基础上给我们提供了更加方便的解决方式，但这种方式在迁移数据库时会给我们带来意想不到的麻烦，所以除非肯定只用一种数据库，否则还是建议按照标准sql书写]]></content>
      <categories>
        <category>sql相关</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>postgresql</tag>
        <tag>greenplum</tag>
      </tags>
  </entry>
</search>
